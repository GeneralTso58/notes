## Faults
Single computer hides the complex physical reality behind a deterministic computing model and prefers crashing completely when things go wrong.
In distributed systems, each node can break undeterministically causing partial failures and you may not even know things failed as the network is also unreliable
**However, building reliable systems out of unreliable components is possible**
To detect faults, You send a message and retry a few times, and set up a timeout, and if no response is heard within timeout, it could be
    - network got cut
    - request got queued
    - request received but scheduled later
    - remote node failed
    - request processed but response lost
    - etc.
If the timeout is too low, we declare a node dead too early, and the follow up operation might stress the system out even more.
If the timeout is too high, we have user wait for too long. It also causes concurrent requests to pile up, which eats up resources and leads to server crashes.
## unreliable network
The variablity of packet delays on computer networks is most often due to queuing
- network switch
- destination machine
- VM monitor
- sender buffer due to TCP congestion control
- lost packet retransmission
- noisy neighbors
### Why don't we make it reliable?
static resource allocation
- circuit-based: allocate a fixed bandwidth along the network link
- wasteful as the usage does not saturate planned resource
dynamic resource partitioning
- suitable for bursty traffic: no way to know how much bandwidth to allocate
- TCP connection does not reserve bandwidth. It dynamically negotiates the transfer on available bandwidth. When idle, it does not use bandwidth
- unfair and unpredictable (unbounded delay) due to queuing
## unreliable clock
Time-of-day clock
- course-grain resolution
- drifts 17 second per day without sync with NTP
- NTP might reset the time making it appear as if moving backward in time
- handles leap-second poorly
monotonic clock
- finer resolution, good for measuring time elapsed
Since local clocks are not reliable, using timestamps generated by those to order writes with LWW will cause dataloss.
### synchronized clocks
Although perfect time is impossible, Spanner reports a confidence interval on the local clock: earliest and latest possible timestamp

## process pause
a node in distributed system must assume its execution can be paused for a significant length of time at any point, even in the middle of function. Providing real time guarantee is inefficient and requires support across stack.
## summary
a node cannot trust its own judgement of a situation.
- use consensus
- fensing tocken: essentially a consensus of current leader term to make sure old leader's writes get rejected whe its term is outdated
## system models
We are mostly dealing with partially synchronuous models that experience crash-recovery faults
safety properties
- nothing bad happens
- required to always hold
liveness properties
- something good eventually happens
- allowed to make caveats